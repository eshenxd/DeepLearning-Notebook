#### 深度学习和机器学习的优化方法汇总
深度学习和机器学习的优化方法包括以下几种模式：
- [Stochastic Gradient Descent (SGD)](#SGD)  
- [Momentum](#Momentum)  
- [AdaGrad](#AdaGrad)   
- [RMSProp](#RMSProp)  
- [Adam](#Adam)  

1. SGD  
  所谓SGD即批量随机梯度下降法，每次随机从训练集中随机挑选特定个数（batch size）个训练样本，成为一个batch，利用该batch训练网络，并将平均梯度回传来更新迭代网络参数。
2. Momentum  
  SGD的缺点是本次更新的方向完全由本次batch决定，因此更新的方向非常不稳定，很可能会导致收敛的时间很长，而Momentum优化的特点是考虑本次更新的方向，同时也会考虑本次batch的优化方向：
  ```mathΔxt = ρ(Δxt−1) − ηgt ```. 其中ρ是动量参数，η是学习率，分别代表偏向于自己优化方向的程度，注意两者之和可以不为1. 一般在训练初期，会考虑将ρ设为0.5，而训练后期则设为0.9。 
3. AdaGrad  
  上述两种方法对所有参数使用了相同的学习率，而不同参数在同一个阶段可能需要调优的幅度是不一样的，因此，AdaGrad方法针对这一点，自适应地为不同参数分配不同的学习率。  
  ![image](http://orxe6lzm4.bkt.clouddn.com/YouDao/1512897305990.png)

4. RMSProp  
  RMSProp方法参考Momentum方法将AdaGrad扩展
  ```
  v += dx^2
  v = b1*v + (1 - b1)*dx^2
  W += -alpha*W * dx/sqrt(v)
  ```

5. Adam  
Adam将Adagrad方法和Momentum方法结合起来：
![image](http://orxe6lzm4.bkt.clouddn.com/YouDao/1512899277041.png)  

6. 各种更新方法在相同训练数据下的收敛路径和速度
![image](img/figure1.gif)
![image](img/figure2.gif)
